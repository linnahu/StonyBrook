ssh -p 22 sjswuhua@hp112.utah.cloudlab.us
ssh -p 22 sjswuhua@hp117.utah.cloudlab.us
ssh -p 22 sjswuhua@hp101.utah.cloudlab.us
# 手动安装JDK
sudo apt-get install -y openjdk-8-jdk
java -version
# 发送本地ssh至3个nodes
scp ~/.ssh/id_rsa* sjswuhua@hp112.utah.cloudlab.us:~/.ssh/
scp ~/.ssh/id_rsa* sjswuhua@hp117.utah.cloudlab.us:~/.ssh/
scp ~/.ssh/id_rsa* sjswuhua@hp101.utah.cloudlab.us:~/.ssh/
# 修改文件及其路径权限
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/id_rsa*
chmod 0600 ~/.ssh/authorized_keys
rm -rf ~/.ssh/known_hosts
echo 'StrictHostKeyChecking no' >> ~/.ssh/config


#下载并解压hadoop
wget https://archive.apache.org/dist/hadoop/core/hadoop-2.7.2/hadoop-2.7.2.tar.gz
tar -xvzf hadoop-2.7.2.tar.gz;  mv hadoop-2.7.2    hadoop
# 设置路径及环境变量
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/ >> .bashrc
echo export HADOOP_PREFIX=~/hadoop >> .bashrc;
echo export HADOOP_YARN_HOME=~/hadoop >> .bashrc;
echo export HADOOP_HOME=~/hadoop >> .bashrc;
echo export HADOOP_CONF_DIR=~/hadoop/etc/hadoop >> .bashrc;
echo export YARN_HOME=~/hadoop >> .bashrc;
echo export YARN_CONF_DIR=~/hadoop/etc/hadoop >> .bashrc;
echo export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin >> .bashrc
source .bashrc
sed -i '1iexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64' hadoop/etc/hadoop/hadoop-env.sh

# 添加临时文件路径
sudo mkdir /dev/hdfs; sudo chmod 777 /dev/hdfs
# 设置hadoop

# 1. core-site.xml
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://ctl:9000/</value>
  </property>

  <property>
    <name>io.file.buffer.size</name>
    <value>65536</value>
  </property>

  <property>
    <name>hadoop.tmp.dir</name>
    <value>/dev/hdfs</value>
  </property>

</configuration>

# 2. hdfs-site.xml
<configuration>

  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>

  <property>
    <name>dfs.blocksize</name>
    <value>134217728</value>
  </property>

  <property>
    <name>dfs.namenode.handler.count</name>
    <value>20</value>
  </property>

</configuration>

# 3. yarn-site.xml
<configuration>
  <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
  </property>
  <property>                                                                  
      <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>  
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>  
  </property>
  
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>hp112.utah.cloudlab.us</value>
  </property>
  <property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
  </property>
  <property>
    <name>yarn.scheduler.fair.allocation.file</name>
    <value>/users/sjswuhua/hadoop/etc/fair-scheduler.xml</value>
  </property>
  <property>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>10</value>
  </property>
  <property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>49152</value>
  </property>
    </property>
  <property>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>16384</value>
  </property>
    <property>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>40962</value>
  </property>
</configuration>

# 4.slaves
cp-1
cp-2

# 5. mapred-site.xml
<configuration>
  <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
  </property>
</configuration>


# 6. fair-scheduler.xml
<?xml version="1.0"?>
<allocations>
<defaultQueueSchedulingPolicy>drf</defaultQueueSchedulingPolicy>
<queue name="queue0">
        <weight>1</weight>
        <allowPreemptionFrom>false</allowPreemptionFrom>
        <schedulingPolicy>fifo</schedulingPolicy>
</queue>
<queue name="queue1">
        <weight>1</weight>
        <allowPreemptionFrom>true</allowPreemptionFrom>
</queue>
</allocations>


# 将设置好的hadoop，分发到slaves
scp -r ~/hadoop sjswuhua@hp117.utah.cloudlab.us:~/hadoop
scp -r ~/hadoop sjswuhua@hp101.utah.cloudlab.us:~/hadoop

**************************************

# 在master启动namenode
hadoop/bin/hdfs namenode -format hdfs
# 启动dfs
hadoop/sbin/start-dfs.sh
# 启动yarn
hadoop/sbin/start-yarn.sh


# 测试1 
hp112.utah.cloudlab.us:50070
hp112.utah.cloudlab.us:8088
jps
# master应当有4个，slaves有3个
# Master: ResourceManager NameNode SecondaryNameNode Jps
# Slaves: NodeManager DataNode Jps

# 测试2
cd hadoop/
bin/hdfs dfs -mkdir -p /test
bin/hdfs dfs -put README.txt /test
bin/hdfs dfs -ls /test
bin/hdfs dfs -cat /test/README.txt
bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount /test /test/result
bin/hdfs dfs -cat /test/result/part-t-00000

********************************************************
# 启动 NameNode
hadoop/sbin/hadoop-daemon.sh start namenode
# 启动 DataNode
hadoop/sbin/hadoop-daemon.sh start datanode
# 启动 SecondaryNameNode
hadoop/sbin/hadoop-daemon.sh start secondarynamenode


# 启动 Resourcemanager
hadoop/sbin/yarn-daemon.sh start resourcemanager
# 启动 Nodemanager
hadoop/sbin/yarn-daemon.sh start nodemanager
# NameNode NodeManager ResourceManager Jps SecondaryNameNode DataNode


yarn application -list -appStates ALL
yarn logs -applicationId
